# -*- coding: utf-8 -*-
"""E_commerce_Users_of_a_French_C2C_fashion_store_ (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V7NFcu9t8ltReSVBagHqSZ_oiPqypn11
"""

import pandas as pd

data = pd.read_csv("/content/6M-0K-99K.users.dataset.public.csv")
data.head()



data.info()

data.head()

data.isnull().sum()

data.describe()

data.columns

df = data.drop("identifierHash" , axis=1)

numcol = df.select_dtypes(include=['int64', 'float64']).columns
numcol

# Skewness
df[numcol].skew()

data[numcol].corr()

import seaborn as sns
import matplotlib.pyplot as plt

# Boxplot for each numerical variable
plt.figure(figsize=(12, 8))
sns.boxplot(data=data[numcol])
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(15, 8))
correlation_matrix = df[numcol].corr()
sns.heatmap(correlation_matrix, annot=True)
plt.show()

data.groupby('language').size().plot(kind='barh')

data.groupby('civilityTitle').size().plot(kind='barh')

country_counts = data.groupby('country').size()
plt.figure(figsize=(12, 30))
country_counts.plot(kind='barh')
plt.xlabel('Count')
plt.ylabel('Country')
plt.title('Number of Observations by Country')
plt.show()

sns.histplot(data['seniorityAsYears'])
plt.title(' Seniority')
plt.xlabel('Seniority')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(data['seniority'])
plt.title('Distribution of Seniority')
plt.xlabel('Seniority')
plt.ylabel('Frequency')
plt.show()

sns.countplot(x='hasAnyApp', data=data)
plt.title('Presence of Any App')
plt.xlabel('Has Any App')
plt.ylabel('Count')
plt.show()

sns.countplot(x='hasAndroidApp', data=data)
plt.title('Presence of Android App')
plt.xlabel('Has Android App')
plt.ylabel('Count')
plt.show()

sns.countplot(x='hasIosApp', data=data)
plt.title('Presence of iOS App')
plt.xlabel('Has iOS App')
plt.ylabel('Count')
plt.show()

a=data['productsSold'].nunique()
a

sns.countplot(x='hasProfilePicture', data=data)
plt.title('Presence of Profile Picture')
plt.xlabel('Has Profile Picture')
plt.ylabel('Count')
plt.show()

plt.scatter(data['socialProductsLiked'], data['productsSold'])
plt.title('Relationship between Social Products Liked and Products Sold')
plt.xlabel('Social Products Liked')
plt.ylabel('Products Sold')
plt.show()

outliers = data[data['socialProductsLiked'] > data['socialProductsLiked'].mean() + 3 * data['socialProductsLiked'].std()]
outliers

data = data.drop(outliers.index)

plt.scatter(data['socialProductsLiked'], data['productsSold'])
plt.title('Relationship between Social Products Liked and Products Sold')
plt.xlabel('Social Products Liked')
plt.ylabel('Products Sold')
plt.show()

data.info()

from scipy import stats
z_scores = pd.DataFrame()
for col in numcol:
    z_scores[col] = stats.zscore(data[col])
z_scores

threshold = 3
outliers = data[(z_scores > threshold).any(axis=1)]
outliers

data = data.drop(outliers.index)

data.info()

bool_columns = data.select_dtypes(include=bool).columns.tolist()
bool_columns

bool_columns = ['hasAnyApp', 'hasAndroidApp', 'hasIosApp', 'hasProfilePicture']
data[bool_columns] = data[bool_columns].astype(int)

data.head()

specific_object_columns = ['gender', 'civilityTitle', 'countryCode']
selected_object_columns = data[specific_object_columns]

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

for col in selected_object_columns.columns:
    data[col] = label_encoder.fit_transform(data[col])

data.info()

data.isna().sum()

data.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

X = data.drop(columns=['productsSold','language','country','type','identifierHash'])
y = data['productsSold']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

linear_reg_pred = linear_reg.predict(X_test)
linear_reg_mse = mean_squared_error(y_test, linear_reg_pred)
linear_reg_r2 = r2_score(y_test, linear_reg_pred)

linear_reg_mse

linear_reg_r2

random_forest_reg = RandomForestRegressor(random_state=42)
random_forest_reg.fit(X_train, y_train)

random_forest_pred = random_forest_reg.predict(X_test)
random_forest_mse = mean_squared_error(y_test, random_forest_pred)
random_forest_r2 = r2_score(y_test, random_forest_pred)

random_forest_mse

random_forest_r2

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

X = data.drop(columns=['productsSold', 'language', 'country', 'type', 'identifierHash'])  # Features
y = data['productsSold']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)
xg_reg.fit(X_train, y_train)

xg_pred = xg_reg.predict(X_test)
xg_mse = mean_squared_error(y_test, xg_pred)
xg_r2 = r2_score(y_test, xg_pred)

xg_mse

xg_r2

data.columns

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

X = data.drop(columns=['productsSold', 'language', 'country', 'type', 'identifierHash'])
Y = data['productsSold']


X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

y_pred = xgb_model.predict(X_test)


accuracy = accuracy_score(y_test, y_pred)
accuracy

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

precision = precision_score(y_test, y_pred, average='weighted')


recall = recall_score(y_test, y_pred, average='weighted')

f1 = f1_score(y_test, y_pred, average='weighted')

conf_matrix = confusion_matrix(y_test, y_pred)


print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", conf_matrix)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

classifiers = {
    "Logistic Regression": LogisticRegression(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Naive Bayes": GaussianNB()
}

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

for clf_name, clf in classifiers.items():
    print("Classifier:", clf_name)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)


    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    conf_matrix = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", conf_matrix)
print()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

from sklearn.model_selection import GridSearchCV

# from sklearn.model_selection import GridSearchCV
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

# # Define the hyperparameter grid
# param_grid_logistic = {
#     'penalty': ['l1', 'l2'],
#     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
#     'solver': ['saga', 'liblinear'],
#     'max_iter': [100, 200, 300, 400, 500, 1000],  # Increased max_iter values
#     'class_weight': [None, 'balanced'],
#     'random_state': [42],
#     'fit_intercept': [True, False],
#     'tol': [1e-4, 1e-3, 1e-2],
# }

# # Create the Logistic Regression model
# logistic_model = LogisticRegression()

# # Create the GridSearchCV object
# logistic_grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid_logistic, cv=5)

# # Fit the GridSearchCV object to the data
# logistic_grid_search.fit(X_train, y_train)

# # Print the best parameters found
# print("Best parameters found for Logistic Regression: ", logistic_grid_search.best_params_)

# # Use the best model to make predictions
# logistic_best_model = logistic_grid_search.best_estimator_
# logistic_y_pred = logistic_best_model.predict(X_test)

# # Calculate evaluation metrics using the best model
# logistic_accuracy = accuracy_score(y_test, logistic_y_pred)
# logistic_conf_matrix = confusion_matrix(y_test, logistic_y_pred)
# logistic_report = classification_report(y_test, logistic_y_pred)
# logistic_auc_roc = roc_auc_score(y_test, logistic_y_pred)

# # Print the evaluation metrics
# print("Logistic Regression Accuracy:", logistic_accuracy)
# print("Logistic Regression Confusion Matrix:\n", logistic_conf_matrix)
# print("Logistic Regression Classification Report:\n", logistic_report)
# print("Logistic Regression AUC-ROC Score:", logistic_auc_roc)

#KNeighborsClassifier
# Define the hyperparameters
hyperparameters = {
    'n_neighbors': 5,              # Number of neighbors
    'weights': 'uniform',          # Weight function used in prediction
    'algorithm': 'auto',           # Algorithm used to compute the nearest neighbors
    'p': 2,                        # Power parameter for the Minkowski metric
    'leaf_size': 30,               # Leaf size passed to BallTree or KDTree
    'metric': 'minkowski'          # Distance metric for tree
}

# Create the KNN model with specified hyperparameters
knn_model = KNeighborsClassifier(**hyperparameters)

# Fit the model to the training data
knn_model.fit(X_train, y_train)

# Make predictions on the test data
knn_y_pred = knn_model.predict(X_test)

# Calculate evaluation metrics
knn_accuracy = accuracy_score(y_test, knn_y_pred)
knn_conf_matrix = confusion_matrix(y_test, knn_y_pred)
knn_report = classification_report(y_test, knn_y_pred)

# Print the evaluation metrics
print("K-Nearest Neighbors Accuracy:", knn_accuracy)
print("K-Nearest Neighbors Confusion Matrix:\n", knn_conf_matrix)
print("K-Nearest Neighbors Classification Report:\n", knn_report)

#LogisticRegression
# Define the hyperparameters
hyperparameters = {
    'penalty': 'l2',                # Penalty (L1 or L2 regularization)
    'C': 1.0,                       # Regularization parameter
    'solver': 'liblinear',          # Algorithm to use in the optimization problem
    'max_iter': 100,                # Maximum number of iterations
    'class_weight': None,           # Weights associated with classes
    'random_state': 42,             # Random state for reproducibility
    'fit_intercept': True,          # Whether to calculate the intercept for this model
    'tol': 0.0001                   # Tolerance for stopping criteria
}

# Create the Logistic Regression model with specified hyperparameters
logistic_model = LogisticRegression(**hyperparameters)

# Fit the model to the training data
logistic_model.fit(X_train, y_train)

# Make predictions on the test data
logistic_y_pred = logistic_model.predict(X_test)

# Calculate evaluation metrics
logistic_accuracy = accuracy_score(y_test, logistic_y_pred)
logistic_conf_matrix = confusion_matrix(y_test, logistic_y_pred)
logistic_report = classification_report(y_test, logistic_y_pred)
print("Logistic Regression Accuracy:", logistic_accuracy)
print("Logistic Regression Confusion Matrix:\n", logistic_conf_matrix)
print("Logistic Regression Classification Report:\n", logistic_report)

#SVC
# Define the hyperparameters
hyperparameters = {
    'C': 1.0,                       # Regularization parameter
    'kernel': 'rbf',                # Kernel type to be used in the algorithm
    'gamma': 'scale',               # Kernel coefficient
    'shrinking': True,              # Whether to use the shrinking heuristic
    'probability': False,           # Whether to enable probability estimates
    'tol': 0.001,                   # Tolerance for stopping criteria
    'max_iter': -1,                 # Maximum number of iterations (-1 for no limit)
    'random_state': 42              # Random state for reproducibility
}

# Create the SVC model with specified hyperparameters
svc_model = SVC(**hyperparameters)

# Fit the model to the training data
svc_model.fit(X_train, y_train)

# Make predictions on the test data
svc_y_pred = svc_model.predict(X_test)

# Calculate evaluation metrics
svc_accuracy = accuracy_score(y_test, svc_y_pred)
svc_conf_matrix = confusion_matrix(y_test, svc_y_pred)
svc_report = classification_report(y_test, svc_y_pred)


print("SVC Accuracy:", svc_accuracy)
print("SVC Confusion Matrix:\n", svc_conf_matrix)
print("SVC Classification Report:\n", svc_report)